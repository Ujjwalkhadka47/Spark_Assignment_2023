{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3TwbP32hcrY"
      },
      "source": [
        "## Working with Different Types of Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFTvfUsWhcrb"
      },
      "source": [
        "### Step 1: Initialize PySpark Session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "npL06Lvchcrb",
        "outputId": "379dc711-6f6e-4298-b652-3ec322418f26"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/09/03 22:39:14 WARN Utils: Your hostname, anujs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.67 instead (on interface en0)\n",
            "23/09/03 22:39:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "23/09/03 22:39:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "# Importing library to create a SparkSession\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Creating a Spark session\n",
        "\n",
        "spark = SparkSession.builder.appName(\"day3\").getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ess267Rhcrd"
      },
      "source": [
        "### Step 2: Load the Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pNVut8Y8hcrd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Load the Chipotle dataset into a Spark DataFrame\n",
        "data_path = \"/Users/anujkhadka/Fusemachines47/ALL SPARK/Assignments/Day_3/titanic.csv\"  \n",
        "titanic_df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
        "\n",
        "# Load the Chipotle dataset into a Spark DataFrame\n",
        "data_path = '/Users/anujkhadka/Fusemachines47/ALL SPARK/Assignments/Day_3/chipotle.csv' \n",
        "chipotle_df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
        "\n",
        "# Load the Chipotle dataset into a Spark DataFrame\n",
        "data_path = '/Users/anujkhadka/Fusemachines47/ALL SPARK/Assignments/Day_3/kalimati_tarkari_dataset.csv' \n",
        "kalimati_df = spark.read.csv(data_path, header=True, inferSchema=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CHZHc48xhcre",
        "outputId": "3c97440e-33b7-45d7-8321-7b7de116ce84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- PassengerId: integer (nullable = true)\n",
            " |-- Survived: integer (nullable = true)\n",
            " |-- Pclass: integer (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Sex: string (nullable = true)\n",
            " |-- Age: double (nullable = true)\n",
            " |-- SibSp: integer (nullable = true)\n",
            " |-- Parch: integer (nullable = true)\n",
            " |-- Ticket: string (nullable = true)\n",
            " |-- Fare: double (nullable = true)\n",
            " |-- Cabin: string (nullable = true)\n",
            " |-- Embarked: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- _c0: integer (nullable = true)\n",
            " |-- order_id: integer (nullable = true)\n",
            " |-- quantity: integer (nullable = true)\n",
            " |-- item_name: string (nullable = true)\n",
            " |-- choice_description: string (nullable = true)\n",
            " |-- item_price: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- SN: integer (nullable = true)\n",
            " |-- Commodity: string (nullable = true)\n",
            " |-- Date: date (nullable = true)\n",
            " |-- Unit: string (nullable = true)\n",
            " |-- Minimum: double (nullable = true)\n",
            " |-- Maximum: double (nullable = true)\n",
            " |-- Average: double (nullable = true)\n",
            "\n",
            "None None None\n"
          ]
        }
      ],
      "source": [
        "#Displaying the schema of all three datasets\n",
        "\n",
        "print(titanic_df.printSchema(),chipotle_df.printSchema(),kalimati_df.printSchema())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF2ULUAmhcre"
      },
      "source": [
        "### Converting to Spark Types:\n",
        "\n",
        "Question: Load the \"titanic\" dataset and convert the \"Fare\" column from double to integer.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR1o70ZZhcre",
        "outputId": "eea0469a-a1d2-48bf-a65d-9023870e7e09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- PassengerId: integer (nullable = true)\n",
            " |-- Survived: integer (nullable = true)\n",
            " |-- Pclass: integer (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Sex: string (nullable = true)\n",
            " |-- Age: double (nullable = true)\n",
            " |-- SibSp: integer (nullable = true)\n",
            " |-- Parch: integer (nullable = true)\n",
            " |-- Ticket: string (nullable = true)\n",
            " |-- Fare: integer (nullable = true)\n",
            " |-- Cabin: string (nullable = true)\n",
            " |-- Embarked: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Expected_Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#My_Solution\n",
        "\n",
        "#Importing the necessary library fro SQL functions\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import IntegerType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- PassengerId: integer (nullable = true)\n",
            " |-- Survived: integer (nullable = true)\n",
            " |-- Pclass: integer (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Sex: string (nullable = true)\n",
            " |-- Age: double (nullable = true)\n",
            " |-- SibSp: integer (nullable = true)\n",
            " |-- Parch: integer (nullable = true)\n",
            " |-- Ticket: string (nullable = true)\n",
            " |-- Fare: integer (nullable = true)\n",
            " |-- Cabin: string (nullable = true)\n",
            " |-- Embarked: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Converting the \"Fare\" column from double to integer.\n",
        "\n",
        "new_titanic_df = titanic_df.withColumn(\"Fare\",col(\"Fare\").cast(\"integer\"))\n",
        "new_titanic_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqhBRIM5hcrf"
      },
      "source": [
        "### Working with Booleans:\n",
        "\n",
        "Question: Load the \"titanic\" dataset and add a new column \"IsAdult\" that indicates whether a passenger is an adult (age >= 18) or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLgAHyqwhcrf",
        "outputId": "b58aed4f-1d29-40f6-f531-c93f12d9f400"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------------------+----+-------+\n",
            "|PassengerId|                Name| Age|IsAdult|\n",
            "+-----------+--------------------+----+-------+\n",
            "|          1|Braund, Mr. Owen ...|22.0|   true|\n",
            "|          2|Cumings, Mrs. Joh...|38.0|   true|\n",
            "|          3|Heikkinen, Miss. ...|26.0|   true|\n",
            "|          4|Futrelle, Mrs. Ja...|35.0|   true|\n",
            "|          5|Allen, Mr. Willia...|35.0|   true|\n",
            "|          6|    Moran, Mr. James|null|  false|\n",
            "|          7|McCarthy, Mr. Tim...|54.0|   true|\n",
            "|          8|Palsson, Master. ...| 2.0|  false|\n",
            "|          9|Johnson, Mrs. Osc...|27.0|   true|\n",
            "|         10|Nasser, Mrs. Nich...|14.0|  false|\n",
            "|         11|Sandstrom, Miss. ...| 4.0|  false|\n",
            "|         12|Bonnell, Miss. El...|58.0|   true|\n",
            "|         13|Saundercock, Mr. ...|20.0|   true|\n",
            "|         14|Andersson, Mr. An...|39.0|   true|\n",
            "|         15|Vestrom, Miss. Hu...|14.0|  false|\n",
            "|         16|Hewlett, Mrs. (Ma...|55.0|   true|\n",
            "|         17|Rice, Master. Eugene| 2.0|  false|\n",
            "|         18|Williams, Mr. Cha...|null|  false|\n",
            "|         19|Vander Planke, Mr...|31.0|   true|\n",
            "|         20|Masselmani, Mrs. ...|null|  false|\n",
            "+-----------+--------------------+----+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Expected_Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------------------+----+-------+\n",
            "|passengerId|                name| Age|IsAdult|\n",
            "+-----------+--------------------+----+-------+\n",
            "|          1|Braund, Mr. Owen ...|22.0|   true|\n",
            "|          2|Cumings, Mrs. Joh...|38.0|   true|\n",
            "|          3|Heikkinen, Miss. ...|26.0|   true|\n",
            "|          4|Futrelle, Mrs. Ja...|35.0|   true|\n",
            "|          5|Allen, Mr. Willia...|35.0|   true|\n",
            "|          6|    Moran, Mr. James|null|   null|\n",
            "|          7|McCarthy, Mr. Tim...|54.0|   true|\n",
            "|          8|Palsson, Master. ...| 2.0|  false|\n",
            "|          9|Johnson, Mrs. Osc...|27.0|   true|\n",
            "|         10|Nasser, Mrs. Nich...|14.0|  false|\n",
            "|         11|Sandstrom, Miss. ...| 4.0|  false|\n",
            "|         12|Bonnell, Miss. El...|58.0|   true|\n",
            "|         13|Saundercock, Mr. ...|20.0|   true|\n",
            "|         14|Andersson, Mr. An...|39.0|   true|\n",
            "|         15|Vestrom, Miss. Hu...|14.0|  false|\n",
            "|         16|Hewlett, Mrs. (Ma...|55.0|   true|\n",
            "|         17|Rice, Master. Eugene| 2.0|  false|\n",
            "|         18|Williams, Mr. Cha...|null|   null|\n",
            "|         19|Vander Planke, Mr...|31.0|   true|\n",
            "|         20|Masselmani, Mrs. ...|null|   null|\n",
            "+-----------+--------------------+----+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#My_Solution\n",
        "\n",
        "#Adding a new column \"IsAdult\" that indicates whether a passenger is an adult (age >= 18) or not\n",
        "\n",
        "ageFilter = col(\"age\") > 18\n",
        "\n",
        "filtered_titanic_df = titanic_df.withColumn(\"IsAdult\", ageFilter)\n",
        "filtered_titanic_df.select(\"passengerId\",\"name\",\"Age\",\"IsAdult\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv3q8oL-hcrf"
      },
      "source": [
        "### Working with Numbers:\n",
        "\n",
        "Question: Load the \"titanic\" dataset and calculate the average age of male and female passengers separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pCk3RMAhcrf",
        "outputId": "935e9f98-7e25-49b9-96cd-3f6f72aa3f1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+------------------+\n",
            "|   Sex|            AvgAge|\n",
            "+------+------------------+\n",
            "|female|27.915708812260537|\n",
            "|  male| 30.72664459161148|\n",
            "+------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Expected_Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+------------------+\n",
            "|   sex|       Average_age|\n",
            "+------+------------------+\n",
            "|female|27.915708812260537|\n",
            "|  male| 30.72664459161148|\n",
            "+------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#My_Solution\n",
        "\n",
        "#Calculating the average age of male and female passengers separately.\n",
        "\n",
        "titanic_df.groupBy(\"sex\").avg(\"age\").withColumnRenamed(\"avg(age)\",\"Average_age\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgBu1Ppahcrf"
      },
      "source": [
        "### Working with Strings:\n",
        "\n",
        "Question: Load the \"chipotle\" dataset and find the item names containing the word \"Chicken.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3chZ7IHbhcrg",
        "outputId": "c46d2562-8820-410d-91ab-fe0b105a6a04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------+--------+--------------------+--------------------+----------+\n",
            "|_c0|order_id|quantity|           item_name|  choice_description|item_price|\n",
            "+---+--------+--------+--------------------+--------------------+----------+\n",
            "|  4|       2|       2|        Chicken Bowl|[Tomatillo-Red Ch...|   $16.98 |\n",
            "|  5|       3|       1|        Chicken Bowl|[Fresh Tomato Sal...|   $10.98 |\n",
            "| 11|       6|       1|Chicken Crispy Tacos|[Roasted Chili Co...|    $8.75 |\n",
            "| 12|       6|       1|  Chicken Soft Tacos|[Roasted Chili Co...|    $8.75 |\n",
            "| 13|       7|       1|        Chicken Bowl|[Fresh Tomato Sal...|   $11.25 |\n",
            "| 16|       8|       1|     Chicken Burrito|[Tomatillo-Green ...|    $8.49 |\n",
            "| 17|       9|       1|     Chicken Burrito|[Fresh Tomato Sal...|    $8.49 |\n",
            "| 19|      10|       1|        Chicken Bowl|[Tomatillo Red Ch...|    $8.75 |\n",
            "| 23|      12|       1|     Chicken Burrito|[[Tomatillo-Green...|   $10.98 |\n",
            "| 26|      13|       1|        Chicken Bowl|[Roasted Chili Co...|    $8.49 |\n",
            "| 29|      15|       1|     Chicken Burrito|[Tomatillo-Green ...|    $8.49 |\n",
            "| 35|      18|       1|  Chicken Soft Tacos|[Roasted Chili Co...|    $8.75 |\n",
            "| 36|      18|       1|  Chicken Soft Tacos|[Roasted Chili Co...|    $8.75 |\n",
            "| 42|      20|       1|        Chicken Bowl|[Roasted Chili Co...|   $11.25 |\n",
            "| 44|      20|       1|  Chicken Salad Bowl|[Fresh Tomato Sal...|    $8.75 |\n",
            "| 45|      21|       1|     Chicken Burrito|[Tomatillo-Red Ch...|   $10.98 |\n",
            "| 52|      24|       1|     Chicken Burrito|[Roasted Chili Co...|   $10.98 |\n",
            "| 63|      28|       1|     Chicken Burrito|[Fresh Tomato Sal...|    $8.75 |\n",
            "| 68|      30|       1|     Chicken Burrito|[Tomatillo-Red Ch...|   $10.98 |\n",
            "| 73|      33|       1|     Chicken Burrito|[Tomatillo Red Ch...|    $8.75 |\n",
            "+---+--------+--------+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/08/21 17:33:29 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
            " Header: , order_id, quantity, item_name, choice_description, item_price\n",
            " Schema: _c0, order_id, quantity, item_name, choice_description, item_price\n",
            "Expected: _c0 but found: \n",
            "CSV file: file:///home/fm-pc-lt-321/Desktop/fuse/pyspark/assignments/trainee%20exercises/data/chipotle.csv\n"
          ]
        }
      ],
      "source": [
        "#Expected_Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "#My_Solution\n",
        "\n",
        "#Importing the necessary library for SQL Function\n",
        "\n",
        "from pyspark.sql.functions import instr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------+--------+--------------------+--------------------+----------+-------------+\n",
            "|_c0|order_id|quantity|           item_name|  choice_description|item_price|chickenFilter|\n",
            "+---+--------+--------+--------------------+--------------------+----------+-------------+\n",
            "|  4|       2|       2|        Chicken Bowl|[Tomatillo-Red Ch...|   $16.98 |         true|\n",
            "|  5|       3|       1|        Chicken Bowl|[Fresh Tomato Sal...|   $10.98 |         true|\n",
            "| 11|       6|       1|Chicken Crispy Tacos|[Roasted Chili Co...|    $8.75 |         true|\n",
            "| 12|       6|       1|  Chicken Soft Tacos|[Roasted Chili Co...|    $8.75 |         true|\n",
            "| 13|       7|       1|        Chicken Bowl|[Fresh Tomato Sal...|   $11.25 |         true|\n",
            "| 16|       8|       1|     Chicken Burrito|[Tomatillo-Green ...|    $8.49 |         true|\n",
            "| 17|       9|       1|     Chicken Burrito|[Fresh Tomato Sal...|    $8.49 |         true|\n",
            "| 19|      10|       1|        Chicken Bowl|[Tomatillo Red Ch...|    $8.75 |         true|\n",
            "| 23|      12|       1|     Chicken Burrito|[[Tomatillo-Green...|   $10.98 |         true|\n",
            "| 26|      13|       1|        Chicken Bowl|[Roasted Chili Co...|    $8.49 |         true|\n",
            "| 29|      15|       1|     Chicken Burrito|[Tomatillo-Green ...|    $8.49 |         true|\n",
            "| 35|      18|       1|  Chicken Soft Tacos|[Roasted Chili Co...|    $8.75 |         true|\n",
            "| 36|      18|       1|  Chicken Soft Tacos|[Roasted Chili Co...|    $8.75 |         true|\n",
            "| 42|      20|       1|        Chicken Bowl|[Roasted Chili Co...|   $11.25 |         true|\n",
            "| 44|      20|       1|  Chicken Salad Bowl|[Fresh Tomato Sal...|    $8.75 |         true|\n",
            "| 45|      21|       1|     Chicken Burrito|[Tomatillo-Red Ch...|   $10.98 |         true|\n",
            "| 52|      24|       1|     Chicken Burrito|[Roasted Chili Co...|   $10.98 |         true|\n",
            "| 63|      28|       1|     Chicken Burrito|[Fresh Tomato Sal...|    $8.75 |         true|\n",
            "| 68|      30|       1|     Chicken Burrito|[Tomatillo-Red Ch...|   $10.98 |         true|\n",
            "| 73|      33|       1|     Chicken Burrito|[Tomatillo Red Ch...|    $8.75 |         true|\n",
            "+---+--------+--------+--------------------+--------------------+----------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/09/03 22:56:56 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
            " Header: , order_id, quantity, item_name, choice_description, item_price\n",
            " Schema: _c0, order_id, quantity, item_name, choice_description, item_price\n",
            "Expected: _c0 but found: \n",
            "CSV file: file:///Users/anujkhadka/Fusemachines47/ALL%20SPARK/Assignments/Day_3/chipotle.csv\n"
          ]
        }
      ],
      "source": [
        "# Calculating the item names containing the word \"Chicken.\n",
        "\n",
        "chickenFilter = instr(col(\"item_name\"),\"Chicken\") >=1\n",
        "chipotle_df.withColumn(\"chickenFilter\",chickenFilter).where(\"chickenFilter\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x6xAqAEhcrg"
      },
      "source": [
        "### Regular Expressions:\n",
        "\n",
        "Question: Load the \"chipotle\" dataset and find the items with names that start with \"Ch\" followed by any character.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPRhqm3Ohcrg",
        "outputId": "4ac202e0-e35d-40e2-fc1f-1fc1657444ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------+--------+--------------------+--------------------+----------+\n",
            "|_c0|order_id|quantity|           item_name|  choice_description|item_price|\n",
            "+---+--------+--------+--------------------+--------------------+----------+\n",
            "|  0|       1|       1|Chips and Fresh T...|                null|    $2.39 |\n",
            "|  3|       1|       1|Chips and Tomatil...|                null|    $2.39 |\n",
            "|  4|       2|       2|        Chicken Bowl|[Tomatillo-Red Ch...|   $16.98 |\n",
            "|  5|       3|       1|        Chicken Bowl|[Fresh Tomato Sal...|   $10.98 |\n",
            "| 10|       5|       1| Chips and Guacamole|                null|    $4.45 |\n",
            "| 11|       6|       1|Chicken Crispy Tacos|[Roasted Chili Co...|    $8.75 |\n",
            "| 12|       6|       1|  Chicken Soft Tacos|[Roasted Chili Co...|    $8.75 |\n",
            "| 13|       7|       1|        Chicken Bowl|[Fresh Tomato Sal...|   $11.25 |\n",
            "| 14|       7|       1| Chips and Guacamole|                null|    $4.45 |\n",
            "| 15|       8|       1|Chips and Tomatil...|                null|    $2.39 |\n",
            "| 16|       8|       1|     Chicken Burrito|[Tomatillo-Green ...|    $8.49 |\n",
            "| 17|       9|       1|     Chicken Burrito|[Fresh Tomato Sal...|    $8.49 |\n",
            "| 19|      10|       1|        Chicken Bowl|[Tomatillo Red Ch...|    $8.75 |\n",
            "| 20|      10|       1| Chips and Guacamole|                null|    $4.45 |\n",
            "| 23|      12|       1|     Chicken Burrito|[[Tomatillo-Green...|   $10.98 |\n",
            "| 25|      13|       1|Chips and Fresh T...|                null|    $2.39 |\n",
            "| 26|      13|       1|        Chicken Bowl|[Roasted Chili Co...|    $8.49 |\n",
            "| 29|      15|       1|     Chicken Burrito|[Tomatillo-Green ...|    $8.49 |\n",
            "| 30|      15|       1|Chips and Tomatil...|                null|    $2.39 |\n",
            "| 35|      18|       1|  Chicken Soft Tacos|[Roasted Chili Co...|    $8.75 |\n",
            "+---+--------+--------+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/08/21 17:35:40 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
            " Header: , order_id, quantity, item_name, choice_description, item_price\n",
            " Schema: _c0, order_id, quantity, item_name, choice_description, item_price\n",
            "Expected: _c0 but found: \n",
            "CSV file: file:///home/fm-pc-lt-321/Desktop/fuse/pyspark/assignments/trainee%20exercises/data/chipotle.csv\n"
          ]
        }
      ],
      "source": [
        "#Expected_Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------+--------+--------------------+--------------------+----------+\n",
            "|_c0|order_id|quantity|           item_name|  choice_description|item_price|\n",
            "+---+--------+--------+--------------------+--------------------+----------+\n",
            "|  0|       1|       1|Chips and Fresh T...|                null|    $2.39 |\n",
            "|  3|       1|       1|Chips and Tomatil...|                null|    $2.39 |\n",
            "|  4|       2|       2|        Chicken Bowl|[Tomatillo-Red Ch...|   $16.98 |\n",
            "|  5|       3|       1|        Chicken Bowl|[Fresh Tomato Sal...|   $10.98 |\n",
            "| 10|       5|       1| Chips and Guacamole|                null|    $4.45 |\n",
            "| 11|       6|       1|Chicken Crispy Tacos|[Roasted Chili Co...|    $8.75 |\n",
            "| 12|       6|       1|  Chicken Soft Tacos|[Roasted Chili Co...|    $8.75 |\n",
            "| 13|       7|       1|        Chicken Bowl|[Fresh Tomato Sal...|   $11.25 |\n",
            "| 14|       7|       1| Chips and Guacamole|                null|    $4.45 |\n",
            "| 15|       8|       1|Chips and Tomatil...|                null|    $2.39 |\n",
            "| 16|       8|       1|     Chicken Burrito|[Tomatillo-Green ...|    $8.49 |\n",
            "| 17|       9|       1|     Chicken Burrito|[Fresh Tomato Sal...|    $8.49 |\n",
            "| 19|      10|       1|        Chicken Bowl|[Tomatillo Red Ch...|    $8.75 |\n",
            "| 20|      10|       1| Chips and Guacamole|                null|    $4.45 |\n",
            "| 23|      12|       1|     Chicken Burrito|[[Tomatillo-Green...|   $10.98 |\n",
            "| 25|      13|       1|Chips and Fresh T...|                null|    $2.39 |\n",
            "| 26|      13|       1|        Chicken Bowl|[Roasted Chili Co...|    $8.49 |\n",
            "| 29|      15|       1|     Chicken Burrito|[Tomatillo-Green ...|    $8.49 |\n",
            "| 30|      15|       1|Chips and Tomatil...|                null|    $2.39 |\n",
            "| 35|      18|       1|  Chicken Soft Tacos|[Roasted Chili Co...|    $8.75 |\n",
            "+---+--------+--------+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/09/03 22:59:19 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
            " Header: , order_id, quantity, item_name, choice_description, item_price\n",
            " Schema: _c0, order_id, quantity, item_name, choice_description, item_price\n",
            "Expected: _c0 but found: \n",
            "CSV file: file:///Users/anujkhadka/Fusemachines47/ALL%20SPARK/Assignments/Day_3/chipotle.csv\n"
          ]
        }
      ],
      "source": [
        "#My_Solution\n",
        "\n",
        "#Finding the items with names that start with \"Ch\" followed by any character\n",
        "\n",
        "# Use a filter with a regular expression to select rows where \"item_name\" starts with \"Ch\" followed by any character\n",
        "filtered_df = chipotle_df.filter(chipotle_df[\"item_name\"].rlike(\"^Ch.\"))\n",
        "\n",
        "# Show the filtered DataFrame\n",
        "filtered_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeoU-Bk-hcrg"
      },
      "source": [
        "### Working with Nulls in Data:\n",
        "\n",
        "Question: Load the \"titanic\" dataset and count the number of passengers with missing age information.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9rRf2fuhcrh",
        "outputId": "7c8d4694-4d20-487f-8d77-944bc711d841"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of passengers with missing age: 177\n"
          ]
        }
      ],
      "source": [
        "#Expected_Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of null values in age column: 177\n"
          ]
        }
      ],
      "source": [
        "#My_Solution\n",
        "\n",
        "# Counting the number of passengers with missing age information\n",
        "\n",
        "countOfNull = titanic_df.filter(col(\"age\").isNull()).count()\n",
        "print(f\"Number of null values in age column: {countOfNull}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IicjcEbQhcrh"
      },
      "source": [
        "### Coalesce\n",
        "Question: Utilizing the Chipotle dataset, use the coalesce function to combine the \"item_name\" and \"choice_description\" columns into a new column named \"OrderDetails.\" Display the first 5 rows of the resulting DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_An091A7hcrh",
        "outputId": "f58c459a-77b9-4718-8a19-c51e807bac6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------+--------+--------------------+--------------------+----------+--------------------+\n",
            "|_c0|order_id|quantity|           item_name|  choice_description|item_price|        OrderDetails|\n",
            "+---+--------+--------+--------------------+--------------------+----------+--------------------+\n",
            "|  0|       1|       1|Chips and Fresh T...|                null|    $2.39 |Chips and Fresh T...|\n",
            "|  1|       1|       1|                Izze|        [Clementine]|    $3.39 |                Izze|\n",
            "|  2|       1|       1|    Nantucket Nectar|             [Apple]|    $3.39 |    Nantucket Nectar|\n",
            "|  3|       1|       1|Chips and Tomatil...|                null|    $2.39 |Chips and Tomatil...|\n",
            "|  4|       2|       2|        Chicken Bowl|[Tomatillo-Red Ch...|   $16.98 |        Chicken Bowl|\n",
            "+---+--------+--------+--------------------+--------------------+----------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/08/21 17:51:49 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
            " Header: , order_id, quantity, item_name, choice_description, item_price\n",
            " Schema: _c0, order_id, quantity, item_name, choice_description, item_price\n",
            "Expected: _c0 but found: \n",
            "CSV file: file:///home/fm-pc-lt-321/Desktop/fuse/pyspark/assignments/trainee%20exercises/data/chipotle.csv\n"
          ]
        }
      ],
      "source": [
        "#Expected_Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "#My_Solution\n",
        "\n",
        "#Importing necessary library for SQL function \"coalesce\"\n",
        "\n",
        "from pyspark.sql.functions import coalesce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------+--------+--------------------+--------------------+----------+--------------------+\n",
            "|_c0|order_id|quantity|           item_name|  choice_description|item_price|        OrderDetails|\n",
            "+---+--------+--------+--------------------+--------------------+----------+--------------------+\n",
            "|  0|       1|       1|Chips and Fresh T...|                null|    $2.39 |Chips and Fresh T...|\n",
            "|  1|       1|       1|                Izze|        [Clementine]|    $3.39 |                Izze|\n",
            "|  2|       1|       1|    Nantucket Nectar|             [Apple]|    $3.39 |    Nantucket Nectar|\n",
            "|  3|       1|       1|Chips and Tomatil...|                null|    $2.39 |Chips and Tomatil...|\n",
            "|  4|       2|       2|        Chicken Bowl|[Tomatillo-Red Ch...|   $16.98 |        Chicken Bowl|\n",
            "+---+--------+--------+--------------------+--------------------+----------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/09/03 23:06:05 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
            " Header: , order_id, quantity, item_name, choice_description, item_price\n",
            " Schema: _c0, order_id, quantity, item_name, choice_description, item_price\n",
            "Expected: _c0 but found: \n",
            "CSV file: file:///Users/anujkhadka/Fusemachines47/ALL%20SPARK/Assignments/Day_3/chipotle.csv\n"
          ]
        }
      ],
      "source": [
        "# Using the coalesce function to combine the \"item_name\" and \"choice_description\" columns into a new column named \"OrderDetails\n",
        "\n",
        "chipotle_df.withColumn(\"OrderDetails\",coalesce(col(\"item_name\"),col(\"choice_description\"))).show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwLFDoxVhcrh"
      },
      "source": [
        "### ifnull, nullIf, nvl, and nvl2\n",
        "\n",
        "Question: Replace the null values in the \"Age\" column of the Titanic dataset with the average age."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ml2AwcqDhcrh",
        "outputId": "3a5923c8-50eb-47fd-e0f0-b8fa700e8376"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+-------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex|              Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+-------+-----+--------+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  male|             22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|female|             38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
            "|          3|       1|     3|Heikkinen, Miss. ...|female|             26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
            "|          4|       1|     1|Futrelle, Mrs. Ja...|female|             35.0|    1|    0|          113803|   53.1| C123|       S|\n",
            "|          5|       0|     3|Allen, Mr. Willia...|  male|             35.0|    0|    0|          373450|   8.05| null|       S|\n",
            "|          6|       0|     3|    Moran, Mr. James|  male|29.69911764705882|    0|    0|          330877| 8.4583| null|       Q|\n",
            "|          7|       0|     1|McCarthy, Mr. Tim...|  male|             54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
            "|          8|       0|     3|Palsson, Master. ...|  male|              2.0|    3|    1|          349909| 21.075| null|       S|\n",
            "|          9|       1|     3|Johnson, Mrs. Osc...|female|             27.0|    0|    2|          347742|11.1333| null|       S|\n",
            "|         10|       1|     2|Nasser, Mrs. Nich...|female|             14.0|    1|    0|          237736|30.0708| null|       C|\n",
            "|         11|       1|     3|Sandstrom, Miss. ...|female|              4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
            "|         12|       1|     1|Bonnell, Miss. El...|female|             58.0|    0|    0|          113783|  26.55| C103|       S|\n",
            "|         13|       0|     3|Saundercock, Mr. ...|  male|             20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
            "|         14|       0|     3|Andersson, Mr. An...|  male|             39.0|    1|    5|          347082| 31.275| null|       S|\n",
            "|         15|       0|     3|Vestrom, Miss. Hu...|female|             14.0|    0|    0|          350406| 7.8542| null|       S|\n",
            "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|             55.0|    0|    0|          248706|   16.0| null|       S|\n",
            "|         17|       0|     3|Rice, Master. Eugene|  male|              2.0|    4|    1|          382652| 29.125| null|       Q|\n",
            "|         18|       1|     2|Williams, Mr. Cha...|  male|29.69911764705882|    0|    0|          244373|   13.0| null|       S|\n",
            "|         19|       0|     3|Vander Planke, Mr...|female|             31.0|    1|    0|          345763|   18.0| null|       S|\n",
            "|         20|       1|     3|Masselmani, Mrs. ...|female|29.69911764705882|    0|    0|            2649|  7.225| null|       C|\n",
            "+-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+-------+-----+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Expected_Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+-------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex|              Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+-------+-----+--------+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  male|             22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|female|             38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
            "|          3|       1|     3|Heikkinen, Miss. ...|female|             26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
            "|          4|       1|     1|Futrelle, Mrs. Ja...|female|             35.0|    1|    0|          113803|   53.1| C123|       S|\n",
            "|          5|       0|     3|Allen, Mr. Willia...|  male|             35.0|    0|    0|          373450|   8.05| null|       S|\n",
            "|          6|       0|     3|    Moran, Mr. James|  male|29.69911764705882|    0|    0|          330877| 8.4583| null|       Q|\n",
            "|          7|       0|     1|McCarthy, Mr. Tim...|  male|             54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
            "|          8|       0|     3|Palsson, Master. ...|  male|              2.0|    3|    1|          349909| 21.075| null|       S|\n",
            "|          9|       1|     3|Johnson, Mrs. Osc...|female|             27.0|    0|    2|          347742|11.1333| null|       S|\n",
            "|         10|       1|     2|Nasser, Mrs. Nich...|female|             14.0|    1|    0|          237736|30.0708| null|       C|\n",
            "|         11|       1|     3|Sandstrom, Miss. ...|female|              4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
            "|         12|       1|     1|Bonnell, Miss. El...|female|             58.0|    0|    0|          113783|  26.55| C103|       S|\n",
            "|         13|       0|     3|Saundercock, Mr. ...|  male|             20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
            "|         14|       0|     3|Andersson, Mr. An...|  male|             39.0|    1|    5|          347082| 31.275| null|       S|\n",
            "|         15|       0|     3|Vestrom, Miss. Hu...|female|             14.0|    0|    0|          350406| 7.8542| null|       S|\n",
            "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|             55.0|    0|    0|          248706|   16.0| null|       S|\n",
            "|         17|       0|     3|Rice, Master. Eugene|  male|              2.0|    4|    1|          382652| 29.125| null|       Q|\n",
            "|         18|       1|     2|Williams, Mr. Cha...|  male|29.69911764705882|    0|    0|          244373|   13.0| null|       S|\n",
            "|         19|       0|     3|Vander Planke, Mr...|female|             31.0|    1|    0|          345763|   18.0| null|       S|\n",
            "|         20|       1|     3|Masselmani, Mrs. ...|female|29.69911764705882|    0|    0|            2649|  7.225| null|       C|\n",
            "+-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+-------+-----+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#My_Solution\n",
        "\n",
        "# Replacing the null values in the \"Age\" column of the Titanic dataset with the average age\n",
        "\n",
        "average_age = titanic_df.agg({'Age':'avg'}).collect()[0][0]\n",
        "average_filled = titanic_df.na.fill(average_age,subset=[\"age\"])\n",
        "average_filled.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4XIS6G-hcri"
      },
      "source": [
        "### drop\n",
        "\n",
        "Question: Remove the \"Cabin\" column from the Titanic dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCsfZznghcri",
        "outputId": "f8ded5c2-ca71-412c-d91e-f805ac115299"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Embarked|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25|       S|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|       C|\n",
            "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925|       S|\n",
            "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1|       S|\n",
            "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05|       S|\n",
            "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583|       Q|\n",
            "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|       S|\n",
            "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075|       S|\n",
            "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333|       S|\n",
            "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708|       C|\n",
            "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|       S|\n",
            "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55|       S|\n",
            "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05|       S|\n",
            "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275|       S|\n",
            "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542|       S|\n",
            "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0|       S|\n",
            "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125|       Q|\n",
            "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0|       S|\n",
            "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0|       S|\n",
            "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225|       C|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Expected_Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Embarked|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25|       S|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|       C|\n",
            "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925|       S|\n",
            "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1|       S|\n",
            "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05|       S|\n",
            "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583|       Q|\n",
            "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|       S|\n",
            "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075|       S|\n",
            "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333|       S|\n",
            "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708|       C|\n",
            "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|       S|\n",
            "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55|       S|\n",
            "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05|       S|\n",
            "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275|       S|\n",
            "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542|       S|\n",
            "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0|       S|\n",
            "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125|       Q|\n",
            "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0|       S|\n",
            "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0|       S|\n",
            "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225|       C|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#My_Solution\n",
        "\n",
        "#Removing the \"Cabin\" column from the Titanic dataset\n",
        "\n",
        "titanic_df.drop(col(\"cabin\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYwDXn6bhcri"
      },
      "source": [
        "### fill\n",
        "\n",
        "Question: Fill the null values in the \"Age\" column of the Titanic dataset with a default age of 30."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynn4NhRPhcri",
        "outputId": "2047c21e-acc3-41aa-90e4-5581e79b4517"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
            "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
            "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
            "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
            "|          6|       0|     3|    Moran, Mr. James|  male|30.0|    0|    0|          330877| 8.4583| null|       Q|\n",
            "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
            "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
            "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
            "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
            "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
            "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
            "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
            "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| null|       S|\n",
            "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| null|       S|\n",
            "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| null|       S|\n",
            "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| null|       Q|\n",
            "|         18|       1|     2|Williams, Mr. Cha...|  male|30.0|    0|    0|          244373|   13.0| null|       S|\n",
            "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| null|       S|\n",
            "|         20|       1|     3|Masselmani, Mrs. ...|female|30.0|    0|    0|            2649|  7.225| null|       C|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Expected_Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
            "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
            "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
            "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
            "|          6|       0|     3|    Moran, Mr. James|  male|30.0|    0|    0|          330877| 8.4583| null|       Q|\n",
            "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
            "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
            "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
            "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
            "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
            "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
            "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
            "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| null|       S|\n",
            "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| null|       S|\n",
            "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| null|       S|\n",
            "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| null|       Q|\n",
            "|         18|       1|     2|Williams, Mr. Cha...|  male|30.0|    0|    0|          244373|   13.0| null|       S|\n",
            "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| null|       S|\n",
            "|         20|       1|     3|Masselmani, Mrs. ...|female|30.0|    0|    0|            2649|  7.225| null|       C|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#My_Solution\n",
        "\n",
        "#Filling the null values in the \"Age\" column of the Titanic dataset with a default age of 30\n",
        "\n",
        "titanic_df.na.fill(30,subset=[\"age\"]).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSg36f8Jhcri"
      },
      "source": [
        "###  replace\n",
        "\n",
        "Question: Replace the gender \"male\" with \"M\" and \"female\" with \"F\" in the \"Sex\" column of the Titanic dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4MfmLODhcri",
        "outputId": "e5fcedad-e1a9-4570-c7c9-c51e73276998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------+------+--------------------+---+----+-----+-----+----------------+-------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+---+----+-----+-----+----------------+-------+-----+--------+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  M|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|  F|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
            "|          3|       1|     3|Heikkinen, Miss. ...|  F|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
            "|          4|       1|     1|Futrelle, Mrs. Ja...|  F|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
            "|          5|       0|     3|Allen, Mr. Willia...|  M|35.0|    0|    0|          373450|   8.05| null|       S|\n",
            "|          6|       0|     3|    Moran, Mr. James|  M|null|    0|    0|          330877| 8.4583| null|       Q|\n",
            "|          7|       0|     1|McCarthy, Mr. Tim...|  M|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
            "|          8|       0|     3|Palsson, Master. ...|  M| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
            "|          9|       1|     3|Johnson, Mrs. Osc...|  F|27.0|    0|    2|          347742|11.1333| null|       S|\n",
            "|         10|       1|     2|Nasser, Mrs. Nich...|  F|14.0|    1|    0|          237736|30.0708| null|       C|\n",
            "|         11|       1|     3|Sandstrom, Miss. ...|  F| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
            "|         12|       1|     1|Bonnell, Miss. El...|  F|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
            "|         13|       0|     3|Saundercock, Mr. ...|  M|20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
            "|         14|       0|     3|Andersson, Mr. An...|  M|39.0|    1|    5|          347082| 31.275| null|       S|\n",
            "|         15|       0|     3|Vestrom, Miss. Hu...|  F|14.0|    0|    0|          350406| 7.8542| null|       S|\n",
            "|         16|       1|     2|Hewlett, Mrs. (Ma...|  F|55.0|    0|    0|          248706|   16.0| null|       S|\n",
            "|         17|       0|     3|Rice, Master. Eugene|  M| 2.0|    4|    1|          382652| 29.125| null|       Q|\n",
            "|         18|       1|     2|Williams, Mr. Cha...|  M|null|    0|    0|          244373|   13.0| null|       S|\n",
            "|         19|       0|     3|Vander Planke, Mr...|  F|31.0|    1|    0|          345763|   18.0| null|       S|\n",
            "|         20|       1|     3|Masselmani, Mrs. ...|  F|null|    0|    0|            2649|  7.225| null|       C|\n",
            "+-----------+--------+------+--------------------+---+----+-----+-----+----------------+-------+-----+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Expected_Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/lib/python3.11/site-packages/pyspark/sql/dataframe.py:4317: UserWarning: to_replace is a dict and value is not None. value will be ignored.\n",
            "  warnings.warn(\"to_replace is a dict and value is not None. value will be ignored.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------+------+--------------------+---+----+-----+-----+----------------+-------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+---+----+-----+-----+----------------+-------+-----+--------+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  m|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|  f|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
            "|          3|       1|     3|Heikkinen, Miss. ...|  f|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
            "|          4|       1|     1|Futrelle, Mrs. Ja...|  f|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
            "|          5|       0|     3|Allen, Mr. Willia...|  m|35.0|    0|    0|          373450|   8.05| null|       S|\n",
            "|          6|       0|     3|    Moran, Mr. James|  m|null|    0|    0|          330877| 8.4583| null|       Q|\n",
            "|          7|       0|     1|McCarthy, Mr. Tim...|  m|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
            "|          8|       0|     3|Palsson, Master. ...|  m| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
            "|          9|       1|     3|Johnson, Mrs. Osc...|  f|27.0|    0|    2|          347742|11.1333| null|       S|\n",
            "|         10|       1|     2|Nasser, Mrs. Nich...|  f|14.0|    1|    0|          237736|30.0708| null|       C|\n",
            "|         11|       1|     3|Sandstrom, Miss. ...|  f| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
            "|         12|       1|     1|Bonnell, Miss. El...|  f|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
            "|         13|       0|     3|Saundercock, Mr. ...|  m|20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
            "|         14|       0|     3|Andersson, Mr. An...|  m|39.0|    1|    5|          347082| 31.275| null|       S|\n",
            "|         15|       0|     3|Vestrom, Miss. Hu...|  f|14.0|    0|    0|          350406| 7.8542| null|       S|\n",
            "|         16|       1|     2|Hewlett, Mrs. (Ma...|  f|55.0|    0|    0|          248706|   16.0| null|       S|\n",
            "|         17|       0|     3|Rice, Master. Eugene|  m| 2.0|    4|    1|          382652| 29.125| null|       Q|\n",
            "|         18|       1|     2|Williams, Mr. Cha...|  m|null|    0|    0|          244373|   13.0| null|       S|\n",
            "|         19|       0|     3|Vander Planke, Mr...|  f|31.0|    1|    0|          345763|   18.0| null|       S|\n",
            "|         20|       1|     3|Masselmani, Mrs. ...|  f|null|    0|    0|            2649|  7.225| null|       C|\n",
            "+-----------+--------+------+--------------------+---+----+-----+-----+----------------+-------+-----+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#My_Solution\n",
        "\n",
        "#Replacing the gender \"male\" with \"M\" and \"female\" with \"F\" in the \"Sex\" column of the Titanic dataset\n",
        "\n",
        "titanic_df.replace({\"male\":\"m\",\"female\":\"f\"},\"sex\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0tKJDozhcrj"
      },
      "source": [
        "### 6. Working with Complex Types: Structs\n",
        "\n",
        "Question: Create a new DataFrame from the Kalimati Tarkari dataset, including a new column \"PriceRange\" that is a struct containing \"Minimum\" and \"Maximum\" prices for each commodity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1oFQiAEhcrj",
        "outputId": "d5b03988-c3c7-48ed-d955-26774b45a9d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------------------+----------+----+-------+-------+-------+------------+\n",
            "|SN |Commodity           |Date      |Unit|Minimum|Maximum|Average|PriceRange  |\n",
            "+---+--------------------+----------+----+-------+-------+-------+------------+\n",
            "|0  |Tomato Big(Nepali)  |2013-06-16|Kg  |35.0   |40.0   |37.5   |{35.0, 40.0}|\n",
            "|1  |Tomato Small(Local) |2013-06-16|Kg  |26.0   |32.0   |29.0   |{26.0, 32.0}|\n",
            "|2  |Potato Red          |2013-06-16|Kg  |20.0   |21.0   |20.5   |{20.0, 21.0}|\n",
            "|3  |Potato White        |2013-06-16|Kg  |15.0   |16.0   |15.5   |{15.0, 16.0}|\n",
            "|4  |Onion Dry (Indian)  |2013-06-16|Kg  |28.0   |30.0   |29.0   |{28.0, 30.0}|\n",
            "|5  |Carrot(Local)       |2013-06-16|Kg  |30.0   |35.0   |32.5   |{30.0, 35.0}|\n",
            "|6  |Cabbage(Local)      |2013-06-16|Kg  |6.0    |10.0   |8.0    |{6.0, 10.0} |\n",
            "|7  |Cauli Local         |2013-06-16|Kg  |30.0   |35.0   |32.5   |{30.0, 35.0}|\n",
            "|8  |Raddish Red         |2013-06-16|Kg  |35.0   |40.0   |37.5   |{35.0, 40.0}|\n",
            "|9  |Raddish White(Local)|2013-06-16|Kg  |25.0   |30.0   |27.5   |{25.0, 30.0}|\n",
            "|10 |Brinjal Long        |2013-06-16|Kg  |16.0   |18.0   |17.0   |{16.0, 18.0}|\n",
            "|11 |Brinjal Round       |2013-06-16|Kg  |20.0   |22.0   |21.0   |{20.0, 22.0}|\n",
            "|12 |Cow pea(Long)       |2013-06-16|Kg  |20.0   |25.0   |22.5   |{20.0, 25.0}|\n",
            "|13 |Green Peas          |2013-06-16|Kg  |55.0   |60.0   |57.5   |{55.0, 60.0}|\n",
            "|14 |French Bean(Local)  |2013-06-16|Kg  |25.0   |30.0   |27.5   |{25.0, 30.0}|\n",
            "|15 |Soyabean Green      |2013-06-16|Kg  |60.0   |70.0   |65.0   |{60.0, 70.0}|\n",
            "|16 |Bitter Gourd        |2013-06-16|Kg  |14.0   |16.0   |15.0   |{14.0, 16.0}|\n",
            "|17 |Bottle Gourd        |2013-06-16|Kg  |15.0   |20.0   |17.5   |{15.0, 20.0}|\n",
            "|18 |Pointed Gourd(Local)|2013-06-16|Kg  |30.0   |35.0   |32.5   |{30.0, 35.0}|\n",
            "|19 |Snake Gourd         |2013-06-16|Kg  |25.0   |30.0   |27.5   |{25.0, 30.0}|\n",
            "+---+--------------------+----------+----+-------+-------+-------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Expected_Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------------------+----------+----+-------+-------+-------+------------+\n",
            "| SN|           Commodity|      Date|Unit|Minimum|Maximum|Average|  PriceRange|\n",
            "+---+--------------------+----------+----+-------+-------+-------+------------+\n",
            "|  0|  Tomato Big(Nepali)|2013-06-16|  Kg|   35.0|   40.0|   37.5|{35.0, 40.0}|\n",
            "|  1| Tomato Small(Local)|2013-06-16|  Kg|   26.0|   32.0|   29.0|{26.0, 32.0}|\n",
            "|  2|          Potato Red|2013-06-16|  Kg|   20.0|   21.0|   20.5|{20.0, 21.0}|\n",
            "|  3|        Potato White|2013-06-16|  Kg|   15.0|   16.0|   15.5|{15.0, 16.0}|\n",
            "|  4|  Onion Dry (Indian)|2013-06-16|  Kg|   28.0|   30.0|   29.0|{28.0, 30.0}|\n",
            "|  5|       Carrot(Local)|2013-06-16|  Kg|   30.0|   35.0|   32.5|{30.0, 35.0}|\n",
            "|  6|      Cabbage(Local)|2013-06-16|  Kg|    6.0|   10.0|    8.0| {6.0, 10.0}|\n",
            "|  7|         Cauli Local|2013-06-16|  Kg|   30.0|   35.0|   32.5|{30.0, 35.0}|\n",
            "|  8|         Raddish Red|2013-06-16|  Kg|   35.0|   40.0|   37.5|{35.0, 40.0}|\n",
            "|  9|Raddish White(Local)|2013-06-16|  Kg|   25.0|   30.0|   27.5|{25.0, 30.0}|\n",
            "| 10|        Brinjal Long|2013-06-16|  Kg|   16.0|   18.0|   17.0|{16.0, 18.0}|\n",
            "| 11|       Brinjal Round|2013-06-16|  Kg|   20.0|   22.0|   21.0|{20.0, 22.0}|\n",
            "| 12|       Cow pea(Long)|2013-06-16|  Kg|   20.0|   25.0|   22.5|{20.0, 25.0}|\n",
            "| 13|          Green Peas|2013-06-16|  Kg|   55.0|   60.0|   57.5|{55.0, 60.0}|\n",
            "| 14|  French Bean(Local)|2013-06-16|  Kg|   25.0|   30.0|   27.5|{25.0, 30.0}|\n",
            "| 15|      Soyabean Green|2013-06-16|  Kg|   60.0|   70.0|   65.0|{60.0, 70.0}|\n",
            "| 16|        Bitter Gourd|2013-06-16|  Kg|   14.0|   16.0|   15.0|{14.0, 16.0}|\n",
            "| 17|        Bottle Gourd|2013-06-16|  Kg|   15.0|   20.0|   17.5|{15.0, 20.0}|\n",
            "| 18|Pointed Gourd(Local)|2013-06-16|  Kg|   30.0|   35.0|   32.5|{30.0, 35.0}|\n",
            "| 19|         Snake Gourd|2013-06-16|  Kg|   25.0|   30.0|   27.5|{25.0, 30.0}|\n",
            "+---+--------------------+----------+----+-------+-------+-------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#My_Solution\n",
        "\n",
        "# Creating a new DataFrame from the Kalimati Tarkari dataset, \n",
        "# including a new column \"PriceRange\" that is a struct containing \"Minimum\" and \"Maximum\" prices for each commodity.\n",
        "\n",
        "compledKalimatiDf = kalimati_df.selectExpr(\"*\",\"struct(Minimum,Maximum) as PriceRange\")\n",
        "compledKalimatiDf.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwZuBZoxhcrj"
      },
      "source": [
        "### Working with Complex Types: Arrays\n",
        "Question: Create a new DataFrame from the Kalimati Tarkari dataset, including a new column \"CommodityList\" that is an array of all the commodities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2WkVVvlhcrj",
        "outputId": "d57f432a-984d-4c1e-e190-6910e58ef9fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------------------+----------+----+-------+-------+-------+----------------------+\n",
            "|SN |Commodity           |Date      |Unit|Minimum|Maximum|Average|CommodityList         |\n",
            "+---+--------------------+----------+----+-------+-------+-------+----------------------+\n",
            "|0  |Tomato Big(Nepali)  |2013-06-16|Kg  |35.0   |40.0   |37.5   |[Tomato Big(Nepali)]  |\n",
            "|1  |Tomato Small(Local) |2013-06-16|Kg  |26.0   |32.0   |29.0   |[Tomato Small(Local)] |\n",
            "|2  |Potato Red          |2013-06-16|Kg  |20.0   |21.0   |20.5   |[Potato Red]          |\n",
            "|3  |Potato White        |2013-06-16|Kg  |15.0   |16.0   |15.5   |[Potato White]        |\n",
            "|4  |Onion Dry (Indian)  |2013-06-16|Kg  |28.0   |30.0   |29.0   |[Onion Dry (Indian)]  |\n",
            "|5  |Carrot(Local)       |2013-06-16|Kg  |30.0   |35.0   |32.5   |[Carrot(Local)]       |\n",
            "|6  |Cabbage(Local)      |2013-06-16|Kg  |6.0    |10.0   |8.0    |[Cabbage(Local)]      |\n",
            "|7  |Cauli Local         |2013-06-16|Kg  |30.0   |35.0   |32.5   |[Cauli Local]         |\n",
            "|8  |Raddish Red         |2013-06-16|Kg  |35.0   |40.0   |37.5   |[Raddish Red]         |\n",
            "|9  |Raddish White(Local)|2013-06-16|Kg  |25.0   |30.0   |27.5   |[Raddish White(Local)]|\n",
            "|10 |Brinjal Long        |2013-06-16|Kg  |16.0   |18.0   |17.0   |[Brinjal Long]        |\n",
            "|11 |Brinjal Round       |2013-06-16|Kg  |20.0   |22.0   |21.0   |[Brinjal Round]       |\n",
            "|12 |Cow pea(Long)       |2013-06-16|Kg  |20.0   |25.0   |22.5   |[Cow pea(Long)]       |\n",
            "|13 |Green Peas          |2013-06-16|Kg  |55.0   |60.0   |57.5   |[Green Peas]          |\n",
            "|14 |French Bean(Local)  |2013-06-16|Kg  |25.0   |30.0   |27.5   |[French Bean(Local)]  |\n",
            "|15 |Soyabean Green      |2013-06-16|Kg  |60.0   |70.0   |65.0   |[Soyabean Green]      |\n",
            "|16 |Bitter Gourd        |2013-06-16|Kg  |14.0   |16.0   |15.0   |[Bitter Gourd]        |\n",
            "|17 |Bottle Gourd        |2013-06-16|Kg  |15.0   |20.0   |17.5   |[Bottle Gourd]        |\n",
            "|18 |Pointed Gourd(Local)|2013-06-16|Kg  |30.0   |35.0   |32.5   |[Pointed Gourd(Local)]|\n",
            "|19 |Snake Gourd         |2013-06-16|Kg  |25.0   |30.0   |27.5   |[Snake Gourd]         |\n",
            "+---+--------------------+----------+----+-------+-------+-------+----------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Expected_Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------------------+----------+----+-------+-------+-------+--------------------+\n",
            "| SN|           Commodity|      Date|Unit|Minimum|Maximum|Average|       CommodityList|\n",
            "+---+--------------------+----------+----+-------+-------+-------+--------------------+\n",
            "|  0|  Tomato Big(Nepali)|2013-06-16|  Kg|   35.0|   40.0|   37.5|[Tomato Big(Nepali)]|\n",
            "|  1| Tomato Small(Local)|2013-06-16|  Kg|   26.0|   32.0|   29.0|[Tomato Small(Loc...|\n",
            "|  2|          Potato Red|2013-06-16|  Kg|   20.0|   21.0|   20.5|        [Potato Red]|\n",
            "|  3|        Potato White|2013-06-16|  Kg|   15.0|   16.0|   15.5|      [Potato White]|\n",
            "|  4|  Onion Dry (Indian)|2013-06-16|  Kg|   28.0|   30.0|   29.0|[Onion Dry (Indian)]|\n",
            "|  5|       Carrot(Local)|2013-06-16|  Kg|   30.0|   35.0|   32.5|     [Carrot(Local)]|\n",
            "|  6|      Cabbage(Local)|2013-06-16|  Kg|    6.0|   10.0|    8.0|    [Cabbage(Local)]|\n",
            "|  7|         Cauli Local|2013-06-16|  Kg|   30.0|   35.0|   32.5|       [Cauli Local]|\n",
            "|  8|         Raddish Red|2013-06-16|  Kg|   35.0|   40.0|   37.5|       [Raddish Red]|\n",
            "|  9|Raddish White(Local)|2013-06-16|  Kg|   25.0|   30.0|   27.5|[Raddish White(Lo...|\n",
            "| 10|        Brinjal Long|2013-06-16|  Kg|   16.0|   18.0|   17.0|      [Brinjal Long]|\n",
            "| 11|       Brinjal Round|2013-06-16|  Kg|   20.0|   22.0|   21.0|     [Brinjal Round]|\n",
            "| 12|       Cow pea(Long)|2013-06-16|  Kg|   20.0|   25.0|   22.5|     [Cow pea(Long)]|\n",
            "| 13|          Green Peas|2013-06-16|  Kg|   55.0|   60.0|   57.5|        [Green Peas]|\n",
            "| 14|  French Bean(Local)|2013-06-16|  Kg|   25.0|   30.0|   27.5|[French Bean(Local)]|\n",
            "| 15|      Soyabean Green|2013-06-16|  Kg|   60.0|   70.0|   65.0|    [Soyabean Green]|\n",
            "| 16|        Bitter Gourd|2013-06-16|  Kg|   14.0|   16.0|   15.0|      [Bitter Gourd]|\n",
            "| 17|        Bottle Gourd|2013-06-16|  Kg|   15.0|   20.0|   17.5|      [Bottle Gourd]|\n",
            "| 18|Pointed Gourd(Local)|2013-06-16|  Kg|   30.0|   35.0|   32.5|[Pointed Gourd(Lo...|\n",
            "| 19|         Snake Gourd|2013-06-16|  Kg|   25.0|   30.0|   27.5|       [Snake Gourd]|\n",
            "+---+--------------------+----------+----+-------+-------+-------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#My_Solution\n",
        "\n",
        "#Creating a new DataFrame from the Kalimati Tarkari dataset, \n",
        "# including a new column \"CommodityList\" that is an array of all the commodities\n",
        "\n",
        "ArrayComplex = kalimati_df.selectExpr(\"*\",\"array(Commodity) as CommodityList\")\n",
        "ArrayComplex.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u86KGKKdhcrj"
      },
      "source": [
        "### Working with Complex Types: explode\n",
        "\n",
        "Question: Explode the \"CommodityList\" array column from the previous step to generate a new row for each commodity in the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XgY-I8ohcrk",
        "outputId": "5a1630fc-1da8-4073-9ff7-8bce358daeed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----------+----+-------+-------+-------+--------------------+\n",
            "|SN |Date      |Unit|Minimum|Maximum|Average|Commodity           |\n",
            "+---+----------+----+-------+-------+-------+--------------------+\n",
            "|0  |2013-06-16|Kg  |35.0   |40.0   |37.5   |Tomato Big(Nepali)  |\n",
            "|1  |2013-06-16|Kg  |26.0   |32.0   |29.0   |Tomato Small(Local) |\n",
            "|2  |2013-06-16|Kg  |20.0   |21.0   |20.5   |Potato Red          |\n",
            "|3  |2013-06-16|Kg  |15.0   |16.0   |15.5   |Potato White        |\n",
            "|4  |2013-06-16|Kg  |28.0   |30.0   |29.0   |Onion Dry (Indian)  |\n",
            "|5  |2013-06-16|Kg  |30.0   |35.0   |32.5   |Carrot(Local)       |\n",
            "|6  |2013-06-16|Kg  |6.0    |10.0   |8.0    |Cabbage(Local)      |\n",
            "|7  |2013-06-16|Kg  |30.0   |35.0   |32.5   |Cauli Local         |\n",
            "|8  |2013-06-16|Kg  |35.0   |40.0   |37.5   |Raddish Red         |\n",
            "|9  |2013-06-16|Kg  |25.0   |30.0   |27.5   |Raddish White(Local)|\n",
            "|10 |2013-06-16|Kg  |16.0   |18.0   |17.0   |Brinjal Long        |\n",
            "|11 |2013-06-16|Kg  |20.0   |22.0   |21.0   |Brinjal Round       |\n",
            "|12 |2013-06-16|Kg  |20.0   |25.0   |22.5   |Cow pea(Long)       |\n",
            "|13 |2013-06-16|Kg  |55.0   |60.0   |57.5   |Green Peas          |\n",
            "|14 |2013-06-16|Kg  |25.0   |30.0   |27.5   |French Bean(Local)  |\n",
            "|15 |2013-06-16|Kg  |60.0   |70.0   |65.0   |Soyabean Green      |\n",
            "|16 |2013-06-16|Kg  |14.0   |16.0   |15.0   |Bitter Gourd        |\n",
            "|17 |2013-06-16|Kg  |15.0   |20.0   |17.5   |Bottle Gourd        |\n",
            "|18 |2013-06-16|Kg  |30.0   |35.0   |32.5   |Pointed Gourd(Local)|\n",
            "|19 |2013-06-16|Kg  |25.0   |30.0   |27.5   |Snake Gourd         |\n",
            "+---+----------+----+-------+-------+-------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Solution_to_be_Exploded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "#My_Solution\n",
        "\n",
        "#Importing necessary library for SQL Function \"explode\"\n",
        "\n",
        "from pyspark.sql.functions import explode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------------------+----------+----+-------+-------+-------+--------------------+--------------------+\n",
            "| SN|           Commodity|      Date|Unit|Minimum|Maximum|Average|       CommodityList|            exploded|\n",
            "+---+--------------------+----------+----+-------+-------+-------+--------------------+--------------------+\n",
            "|  0|  Tomato Big(Nepali)|2013-06-16|  Kg|   35.0|   40.0|   37.5|[Tomato Big(Nepali)]|  Tomato Big(Nepali)|\n",
            "|  1| Tomato Small(Local)|2013-06-16|  Kg|   26.0|   32.0|   29.0|[Tomato Small(Loc...| Tomato Small(Local)|\n",
            "|  2|          Potato Red|2013-06-16|  Kg|   20.0|   21.0|   20.5|        [Potato Red]|          Potato Red|\n",
            "|  3|        Potato White|2013-06-16|  Kg|   15.0|   16.0|   15.5|      [Potato White]|        Potato White|\n",
            "|  4|  Onion Dry (Indian)|2013-06-16|  Kg|   28.0|   30.0|   29.0|[Onion Dry (Indian)]|  Onion Dry (Indian)|\n",
            "|  5|       Carrot(Local)|2013-06-16|  Kg|   30.0|   35.0|   32.5|     [Carrot(Local)]|       Carrot(Local)|\n",
            "|  6|      Cabbage(Local)|2013-06-16|  Kg|    6.0|   10.0|    8.0|    [Cabbage(Local)]|      Cabbage(Local)|\n",
            "|  7|         Cauli Local|2013-06-16|  Kg|   30.0|   35.0|   32.5|       [Cauli Local]|         Cauli Local|\n",
            "|  8|         Raddish Red|2013-06-16|  Kg|   35.0|   40.0|   37.5|       [Raddish Red]|         Raddish Red|\n",
            "|  9|Raddish White(Local)|2013-06-16|  Kg|   25.0|   30.0|   27.5|[Raddish White(Lo...|Raddish White(Local)|\n",
            "| 10|        Brinjal Long|2013-06-16|  Kg|   16.0|   18.0|   17.0|      [Brinjal Long]|        Brinjal Long|\n",
            "| 11|       Brinjal Round|2013-06-16|  Kg|   20.0|   22.0|   21.0|     [Brinjal Round]|       Brinjal Round|\n",
            "| 12|       Cow pea(Long)|2013-06-16|  Kg|   20.0|   25.0|   22.5|     [Cow pea(Long)]|       Cow pea(Long)|\n",
            "| 13|          Green Peas|2013-06-16|  Kg|   55.0|   60.0|   57.5|        [Green Peas]|          Green Peas|\n",
            "| 14|  French Bean(Local)|2013-06-16|  Kg|   25.0|   30.0|   27.5|[French Bean(Local)]|  French Bean(Local)|\n",
            "| 15|      Soyabean Green|2013-06-16|  Kg|   60.0|   70.0|   65.0|    [Soyabean Green]|      Soyabean Green|\n",
            "| 16|        Bitter Gourd|2013-06-16|  Kg|   14.0|   16.0|   15.0|      [Bitter Gourd]|        Bitter Gourd|\n",
            "| 17|        Bottle Gourd|2013-06-16|  Kg|   15.0|   20.0|   17.5|      [Bottle Gourd]|        Bottle Gourd|\n",
            "| 18|Pointed Gourd(Local)|2013-06-16|  Kg|   30.0|   35.0|   32.5|[Pointed Gourd(Lo...|Pointed Gourd(Local)|\n",
            "| 19|         Snake Gourd|2013-06-16|  Kg|   25.0|   30.0|   27.5|       [Snake Gourd]|         Snake Gourd|\n",
            "+---+--------------------+----------+----+-------+-------+-------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Explode the \"CommodityList\" array column from the previous step to generate a new row for each commodity in the list\n",
        "\n",
        "#Exploded Solution\n",
        "\n",
        "ArrayComplex.withColumn(\"exploded\",explode(\"CommodityList\")).show(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11oouDUohcrk"
      },
      "source": [
        "### Working with Complex Types: Maps\n",
        "\n",
        "Question: Create a new DataFrame from the Kalimati Tarkari dataset, including a new column \"PriceMap\" that is a map with \"Commodity\" as the key and \"Average\" price as the value.\n",
        "Answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmBc1Gqohcrk",
        "outputId": "ef1695a6-b7b8-420e-d550-9408cf2aaea0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------------------+----------+----+-------+-------+-------+------------------------------+\n",
            "|SN |Commodity           |Date      |Unit|Minimum|Maximum|Average|PriceMap                      |\n",
            "+---+--------------------+----------+----+-------+-------+-------+------------------------------+\n",
            "|0  |Tomato Big(Nepali)  |2013-06-16|Kg  |35.0   |40.0   |37.5   |{Tomato Big(Nepali) -> 37.5}  |\n",
            "|1  |Tomato Small(Local) |2013-06-16|Kg  |26.0   |32.0   |29.0   |{Tomato Small(Local) -> 29.0} |\n",
            "|2  |Potato Red          |2013-06-16|Kg  |20.0   |21.0   |20.5   |{Potato Red -> 20.5}          |\n",
            "|3  |Potato White        |2013-06-16|Kg  |15.0   |16.0   |15.5   |{Potato White -> 15.5}        |\n",
            "|4  |Onion Dry (Indian)  |2013-06-16|Kg  |28.0   |30.0   |29.0   |{Onion Dry (Indian) -> 29.0}  |\n",
            "|5  |Carrot(Local)       |2013-06-16|Kg  |30.0   |35.0   |32.5   |{Carrot(Local) -> 32.5}       |\n",
            "|6  |Cabbage(Local)      |2013-06-16|Kg  |6.0    |10.0   |8.0    |{Cabbage(Local) -> 8.0}       |\n",
            "|7  |Cauli Local         |2013-06-16|Kg  |30.0   |35.0   |32.5   |{Cauli Local -> 32.5}         |\n",
            "|8  |Raddish Red         |2013-06-16|Kg  |35.0   |40.0   |37.5   |{Raddish Red -> 37.5}         |\n",
            "|9  |Raddish White(Local)|2013-06-16|Kg  |25.0   |30.0   |27.5   |{Raddish White(Local) -> 27.5}|\n",
            "|10 |Brinjal Long        |2013-06-16|Kg  |16.0   |18.0   |17.0   |{Brinjal Long -> 17.0}        |\n",
            "|11 |Brinjal Round       |2013-06-16|Kg  |20.0   |22.0   |21.0   |{Brinjal Round -> 21.0}       |\n",
            "|12 |Cow pea(Long)       |2013-06-16|Kg  |20.0   |25.0   |22.5   |{Cow pea(Long) -> 22.5}       |\n",
            "|13 |Green Peas          |2013-06-16|Kg  |55.0   |60.0   |57.5   |{Green Peas -> 57.5}          |\n",
            "|14 |French Bean(Local)  |2013-06-16|Kg  |25.0   |30.0   |27.5   |{French Bean(Local) -> 27.5}  |\n",
            "|15 |Soyabean Green      |2013-06-16|Kg  |60.0   |70.0   |65.0   |{Soyabean Green -> 65.0}      |\n",
            "|16 |Bitter Gourd        |2013-06-16|Kg  |14.0   |16.0   |15.0   |{Bitter Gourd -> 15.0}        |\n",
            "|17 |Bottle Gourd        |2013-06-16|Kg  |15.0   |20.0   |17.5   |{Bottle Gourd -> 17.5}        |\n",
            "|18 |Pointed Gourd(Local)|2013-06-16|Kg  |30.0   |35.0   |32.5   |{Pointed Gourd(Local) -> 32.5}|\n",
            "|19 |Snake Gourd         |2013-06-16|Kg  |25.0   |30.0   |27.5   |{Snake Gourd -> 27.5}         |\n",
            "+---+--------------------+----------+----+-------+-------+-------+------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Expected_Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "#My_Solution\n",
        "\n",
        "#Importing necessary library for SQL Function \"create_map\"\n",
        "\n",
        "from pyspark.sql.functions import create_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------------------+----------+----+-------+-------+-------+--------------------+\n",
            "| SN|           Commodity|      Date|Unit|Minimum|Maximum|Average|            PriceMap|\n",
            "+---+--------------------+----------+----+-------+-------+-------+--------------------+\n",
            "|  0|  Tomato Big(Nepali)|2013-06-16|  Kg|   35.0|   40.0|   37.5|{Tomato Big(Nepal...|\n",
            "|  1| Tomato Small(Local)|2013-06-16|  Kg|   26.0|   32.0|   29.0|{Tomato Small(Loc...|\n",
            "|  2|          Potato Red|2013-06-16|  Kg|   20.0|   21.0|   20.5|{Potato Red -> 20.5}|\n",
            "|  3|        Potato White|2013-06-16|  Kg|   15.0|   16.0|   15.5|{Potato White -> ...|\n",
            "|  4|  Onion Dry (Indian)|2013-06-16|  Kg|   28.0|   30.0|   29.0|{Onion Dry (India...|\n",
            "|  5|       Carrot(Local)|2013-06-16|  Kg|   30.0|   35.0|   32.5|{Carrot(Local) ->...|\n",
            "|  6|      Cabbage(Local)|2013-06-16|  Kg|    6.0|   10.0|    8.0|{Cabbage(Local) -...|\n",
            "|  7|         Cauli Local|2013-06-16|  Kg|   30.0|   35.0|   32.5|{Cauli Local -> 3...|\n",
            "|  8|         Raddish Red|2013-06-16|  Kg|   35.0|   40.0|   37.5|{Raddish Red -> 3...|\n",
            "|  9|Raddish White(Local)|2013-06-16|  Kg|   25.0|   30.0|   27.5|{Raddish White(Lo...|\n",
            "| 10|        Brinjal Long|2013-06-16|  Kg|   16.0|   18.0|   17.0|{Brinjal Long -> ...|\n",
            "| 11|       Brinjal Round|2013-06-16|  Kg|   20.0|   22.0|   21.0|{Brinjal Round ->...|\n",
            "| 12|       Cow pea(Long)|2013-06-16|  Kg|   20.0|   25.0|   22.5|{Cow pea(Long) ->...|\n",
            "| 13|          Green Peas|2013-06-16|  Kg|   55.0|   60.0|   57.5|{Green Peas -> 57.5}|\n",
            "| 14|  French Bean(Local)|2013-06-16|  Kg|   25.0|   30.0|   27.5|{French Bean(Loca...|\n",
            "| 15|      Soyabean Green|2013-06-16|  Kg|   60.0|   70.0|   65.0|{Soyabean Green -...|\n",
            "| 16|        Bitter Gourd|2013-06-16|  Kg|   14.0|   16.0|   15.0|{Bitter Gourd -> ...|\n",
            "| 17|        Bottle Gourd|2013-06-16|  Kg|   15.0|   20.0|   17.5|{Bottle Gourd -> ...|\n",
            "| 18|Pointed Gourd(Local)|2013-06-16|  Kg|   30.0|   35.0|   32.5|{Pointed Gourd(Lo...|\n",
            "| 19|         Snake Gourd|2013-06-16|  Kg|   25.0|   30.0|   27.5|{Snake Gourd -> 2...|\n",
            "+---+--------------------+----------+----+-------+-------+-------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Creating a new DataFrame from the Kalimati Tarkari dataset, \n",
        "# including a new column \"PriceMap\" that is a map with \"Commodity\" as the key and \"Average\" price as the value.\n",
        "\n",
        "MapsKalimati = kalimati_df.select(\"*\",create_map(col(\"Commodity\"),col(\"Average\")).alias(\"PriceMap\"))\n",
        "MapsKalimati.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQgKZbAEhcrl"
      },
      "source": [
        "### Working with JSON\n",
        "\n",
        "Question: Convert the \"kalimati_df\" DataFrame to JSON format and write it to a JSON file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "XCVH23ishcrl"
      },
      "outputs": [],
      "source": [
        "#Importing necessary library for \"to_json & from_json\"\n",
        "\n",
        "from pyspark.sql.functions import to_json,from_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "jmeC-19Khcrl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------+\n",
            "|to_json(KalimatiJson)|\n",
            "+---------------------+\n",
            "| {\"Sn\":0,\"Commodit...|\n",
            "| {\"Sn\":1,\"Commodit...|\n",
            "| {\"Sn\":2,\"Commodit...|\n",
            "| {\"Sn\":3,\"Commodit...|\n",
            "| {\"Sn\":4,\"Commodit...|\n",
            "| {\"Sn\":5,\"Commodit...|\n",
            "| {\"Sn\":6,\"Commodit...|\n",
            "| {\"Sn\":7,\"Commodit...|\n",
            "| {\"Sn\":8,\"Commodit...|\n",
            "| {\"Sn\":9,\"Commodit...|\n",
            "| {\"Sn\":10,\"Commodi...|\n",
            "| {\"Sn\":11,\"Commodi...|\n",
            "| {\"Sn\":12,\"Commodi...|\n",
            "| {\"Sn\":13,\"Commodi...|\n",
            "| {\"Sn\":14,\"Commodi...|\n",
            "| {\"Sn\":15,\"Commodi...|\n",
            "| {\"Sn\":16,\"Commodi...|\n",
            "| {\"Sn\":17,\"Commodi...|\n",
            "| {\"Sn\":18,\"Commodi...|\n",
            "| {\"Sn\":19,\"Commodi...|\n",
            "+---------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Converting the \"kalimati_df\" DataFrame to JSON format and write it to a JSON file\n",
        "\n",
        "jsonKalimati = kalimati_df.selectExpr(\"(Sn,Commodity,date,unit,minimum,maximum,average) as KalimatiJson\").select(to_json(col(\"KalimatiJson\")))\n",
        "jsonKalimati.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
