{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOviCBhbngLe"
      },
      "source": [
        "# Ex1 - Getting and Knowing your Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnVjy8OzngLf"
      },
      "source": [
        "\n",
        "### Step 1: Initialize PySpark Session\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BWKS0Y-MngLg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/09/04 15:37:57 WARN Utils: Your hostname, anujs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.68 instead (on interface en0)\n",
            "23/09/04 15:37:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "23/09/04 15:37:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "#Import the spark library ti initiate SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"Day1\").getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsKpvtp0ngLh"
      },
      "source": [
        "### Step 2: Load the Dataset\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8HqS9YFLngLh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/09/04 15:38:10 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
          ]
        }
      ],
      "source": [
        "# Load the Chipotle dataset into a Spark DataFrame\n",
        "data_path = '/Users/anujkhadka/Fusemachines47/ALL SPARK/Spark_Assignment_2023/Day_1/chipotle.csv' # Replace with the actual path\n",
        "df = spark.read.csv(data_path, header=True, inferSchema=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPy0RQ34ngLh"
      },
      "source": [
        "### Step 3. Get an overview of the DataFrame:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPQhClvongLh",
        "outputId": "4f792aac-b98f-48d1-95fa-907fdc9161f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- _c0: integer (nullable = true)\n",
            " |-- order_id: integer (nullable = true)\n",
            " |-- quantity: integer (nullable = true)\n",
            " |-- item_name: string (nullable = true)\n",
            " |-- choice_description: string (nullable = true)\n",
            " |-- item_price: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#To get overview of the dataframe\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENzcdIl6ngLh",
        "outputId": "5e696043-b658-4b1a-d055-83a784440f25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------+--------+--------------------+--------------------+----------+\n",
            "|_c0|order_id|quantity|           item_name|  choice_description|item_price|\n",
            "+---+--------+--------+--------------------+--------------------+----------+\n",
            "|  0|       1|       1|Chips and Fresh T...|                null|    $2.39 |\n",
            "|  1|       1|       1|                Izze|        [Clementine]|    $3.39 |\n",
            "|  2|       1|       1|    Nantucket Nectar|             [Apple]|    $3.39 |\n",
            "|  3|       1|       1|Chips and Tomatil...|                null|    $2.39 |\n",
            "|  4|       2|       2|        Chicken Bowl|[Tomatillo-Red Ch...|   $16.98 |\n",
            "+---+--------+--------+--------------------+--------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/09/04 15:38:57 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
            " Header: , order_id, quantity, item_name, choice_description, item_price\n",
            " Schema: _c0, order_id, quantity, item_name, choice_description, item_price\n",
            "Expected: _c0 but found: \n",
            "CSV file: file:///Users/anujkhadka/Fusemachines47/ALL%20SPARK/Spark_Assignment_2023/Day_1/chipotle.csv\n"
          ]
        }
      ],
      "source": [
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liKiPfy8ngLh"
      },
      "source": [
        "### Step 4.Calculate basic statistics:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucLkr2RwngLh",
        "outputId": "9883833d-c71c-4efb-d981-57f18e0fe0ec",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23/09/04 15:39:04 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "23/09/04 15:39:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
            " Header: , order_id, quantity, item_name, choice_description, item_price\n",
            " Schema: _c0, order_id, quantity, item_name, choice_description, item_price\n",
            "Expected: _c0 but found: \n",
            "CSV file: file:///Users/anujkhadka/Fusemachines47/ALL%20SPARK/Spark_Assignment_2023/Day_1/chipotle.csv\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------------+-----------------+------------------+-----------------+--------------------+----------+\n",
            "|summary|               _c0|         order_id|          quantity|        item_name|  choice_description|item_price|\n",
            "+-------+------------------+-----------------+------------------+-----------------+--------------------+----------+\n",
            "|  count|              4622|             4622|              4622|             4622|                3376|      4622|\n",
            "|   mean|            2310.5|927.2548680225011|1.0757247944612722|             null|                null|      null|\n",
            "| stddev|1334.4008018582722|528.8907955866096|0.4101863342575333|             null|                null|      null|\n",
            "|    min|                 0|                1|                 1|6 Pack Soft Drink|[Adobo-Marinated ...|    $1.09 |\n",
            "|    max|              4621|             1834|                15|Veggie Soft Tacos|[[Tomatillo-Red C...|    $9.39 |\n",
            "+-------+------------------+-----------------+------------------+-----------------+--------------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.describe().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7UQ2Yg5ngLh"
      },
      "source": [
        "### Step 5. What is the number of observations in the dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J37y3eu0ngLh",
        "outputId": "1d9948a6-00c4-42f9-98b6-92a67819bf91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of observation: 4622\n"
          ]
        }
      ],
      "source": [
        "#To calculate no. of observations in the dataset\n",
        "\n",
        "count = df.count()\n",
        "print(f\"Number of observation: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0fc1IqnngLh"
      },
      "source": [
        "### Step 6. What is the number of columns in the dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvrsnxdnngLh",
        "outputId": "9f4a376e-d653-41ec-c1fb-0b4970611eac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of columns: 6\n"
          ]
        }
      ],
      "source": [
        "#To get number of columns in the dataset\n",
        "print(f\"Number of columns: {len(df.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6UHKvXRngLi"
      },
      "source": [
        "### Step 7. Print the name of all the columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Umq6X5rongLi",
        "outputId": "ecc482c6-3f07-48e9-d688-9824288b9ab8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['_c0',\n",
              " 'order_id',\n",
              " 'quantity',\n",
              " 'item_name',\n",
              " 'choice_description',\n",
              " 'item_price']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ypagj_dQMwa6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sSjjrDsnMxiG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
